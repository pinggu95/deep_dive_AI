{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinggu95/deep_dive_AI/blob/main/63%EC%9D%BC%EC%B0%A8/63%EC%9D%BC%EC%B0%A8_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b294b3be",
      "metadata": {
        "id": "b294b3be"
      },
      "source": [
        "# 벡터스토어 기반 검색기(VectorStore-backed Retriever)\n",
        "\n",
        "**VectorStore 지원 검색기** 는 vector store를 사용하여 문서를 검색하는 retriever입니다.\n",
        "\n",
        "Vector store에 구현된 **유사도 검색(similarity search)** 이나 **MMR** 과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc586a1",
      "metadata": {
        "id": "1dc586a1"
      },
      "source": [
        "아래의 코드를 실행하여 VectorStore 를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeYKRH1up2K2",
        "outputId": "3a8aa94b-b778-478f-88c7-6a73ed433014"
      },
      "id": "XeYKRH1up2K2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "137bd65d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137bd65d",
        "outputId": "ac00f24c-364a-46b8-abf0-86c4cc538c67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# API 키 정보 로드\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "329e78b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "329e78b6",
        "outputId": "a713cee2-fe67-4597-d14e-5447c5e729c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-teddynote\n",
            "  Downloading langchain_teddynote-0.4.4-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting anthropic>=0.64.0 (from langchain-teddynote)\n",
            "  Downloading anthropic-0.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting deepl>=1.22.0 (from langchain-teddynote)\n",
            "  Downloading deepl-1.22.0-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting feedparser>=6.0.11 (from langchain-teddynote)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting kiwipiepy>=0.21.0 (from langchain-teddynote)\n",
            "  Downloading kiwipiepy-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting konlpy>=0.6.0 (from langchain-teddynote)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-openai>=0.3.30 (from langchain-teddynote)\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-teddynote) (0.3.27)\n",
            "Collecting langgraph>=0.6.5 (from langchain-teddynote)\n",
            "  Downloading langgraph-0.6.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting olefile>=0.47 (from langchain-teddynote)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: openai>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain-teddynote) (1.99.9)\n",
            "Collecting pandas>=2.3.1 (from langchain-teddynote)\n",
            "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image>=1.17.0 (from langchain-teddynote)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pinecone-client>=6.0.0 (from langchain-teddynote)\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pinecone-text>=0.11.0 (from langchain-teddynote)\n",
            "  Downloading pinecone_text-0.11.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dotenv>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from langchain-teddynote) (1.1.1)\n",
            "Collecting rank-bm25>=0.2.2 (from langchain-teddynote)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting tavily-python>=0.7.10 (from langchain-teddynote)\n",
            "  Downloading tavily_python-0.7.10-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting twine>=6.1.0 (from langchain-teddynote)\n",
            "  Downloading twine-6.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic>=0.64.0->langchain-teddynote) (4.14.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from deepl>=1.22.0->langchain-teddynote) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser>=6.0.11->langchain-teddynote)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kiwipiepy_model<0.22,>=0.21 (from kiwipiepy>=0.21.0->langchain-teddynote)\n",
            "  Downloading kiwipiepy_model-0.21.0.tar.gz (35.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kiwipiepy>=0.21.0->langchain-teddynote) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kiwipiepy>=0.21.0->langchain-teddynote) (4.67.1)\n",
            "Collecting JPype1>=0.7.0 (from konlpy>=0.6.0->langchain-teddynote)\n",
            "  Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy>=0.6.0->langchain-teddynote) (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3.27->langchain-teddynote) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3.27->langchain-teddynote) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3.27->langchain-teddynote) (0.4.14)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3.27->langchain-teddynote) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3.27->langchain-teddynote) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.30->langchain-teddynote) (0.11.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph>=0.6.5->langchain-teddynote)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph>=0.6.5->langchain-teddynote)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph>=0.6.5->langchain-teddynote)\n",
            "  Downloading langgraph_sdk-0.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph>=0.6.5->langchain-teddynote) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.3.1->langchain-teddynote) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.3.1->langchain-teddynote) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.3.1->langchain-teddynote) (2025.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image>=1.17.0->langchain-teddynote) (11.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client>=6.0.0->langchain-teddynote) (2025.8.3)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client>=6.0.0->langchain-teddynote)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client>=6.0.0->langchain-teddynote) (2.5.0)\n",
            "Collecting mmh3<5.0.0,>=4.1.0 (from pinecone-text>=0.11.0->langchain-teddynote)\n",
            "  Downloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-text>=0.11.0->langchain-teddynote) (3.9.1)\n",
            "Collecting types-requests<3.0.0,>=2.25.0 (from pinecone-text>=0.11.0->langchain-teddynote)\n",
            "  Downloading types_requests-2.32.4.20250809-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine>=6.1.0->langchain-teddynote)\n",
            "  Downloading readme_renderer-44.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from twine>=6.1.0->langchain-teddynote) (1.0.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/local/lib/python3.11/dist-packages (from twine>=6.1.0->langchain-teddynote) (25.6.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine>=6.1.0->langchain-teddynote)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from twine>=6.1.0->langchain-teddynote) (13.9.4)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.11/dist-packages (from twine>=6.1.0->langchain-teddynote) (25.0)\n",
            "Collecting id (from twine>=6.1.0->langchain-teddynote)\n",
            "  Downloading id-1.5.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic>=0.64.0->langchain-teddynote) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic>=0.64.0->langchain-teddynote) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic>=0.64.0->langchain-teddynote) (0.16.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (0.9.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.11.4 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (8.7.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (4.2.1)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine>=6.1.0->langchain-teddynote) (6.0.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.3.27->langchain-teddynote) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.3.27->langchain-teddynote) (1.33)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph>=0.6.5->langchain-teddynote)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph>=0.6.5->langchain-teddynote) (3.11.2)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.3.27->langchain-teddynote) (0.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.9.1->pinecone-text>=0.11.0->langchain-teddynote) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.9.1->pinecone-text>=0.11.0->langchain-teddynote) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.9.1->pinecone-text>=0.11.0->langchain-teddynote) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic>=0.64.0->langchain-teddynote) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic>=0.64.0->langchain-teddynote) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic>=0.64.0->langchain-teddynote) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.3.1->langchain-teddynote) (1.17.0)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine>=6.1.0->langchain-teddynote)\n",
            "  Downloading nh3-0.3.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: docutils>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine>=6.1.0->langchain-teddynote) (0.21.2)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine>=6.1.0->langchain-teddynote) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->deepl>=1.22.0->langchain-teddynote) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->twine>=6.1.0->langchain-teddynote) (4.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3.27->langchain-teddynote) (3.2.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.11.4->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.3.27->langchain-teddynote) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine>=6.1.0->langchain-teddynote) (0.1.2)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.11/dist-packages (from SecretStorage>=3.2->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (10.7.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine>=6.1.0->langchain-teddynote) (2.22)\n",
            "Downloading langchain_teddynote-0.4.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.64.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepl-1.22.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwipiepy-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.5-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading pinecone_text-0.11.0-py3-none-any.whl (22 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tavily_python-0.7.10-py3-none-any.whl (15 kB)\n",
            "Downloading twine-6.1.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.6/496.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading readme_renderer-44.0-py3-none-any.whl (13 kB)\n",
            "Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading types_requests-2.32.4.20250809-py3-none-any.whl (20 kB)\n",
            "Downloading id-1.5.0-py3-none-any.whl (13 kB)\n",
            "Downloading nh3-0.3.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kiwipiepy_model, sgmllib3k\n",
            "  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.21.0-py3-none-any.whl size=35593192 sha256=0099bfc90096c28d54b9618a02a39bb1be883aed6692d04e127a707d59bb4e0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/16/3d/95053ab5298f0f0f22ffea6de0200b6f24bffb73cab4c1a828\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=09bf9835914ea5e27bffe3a82da3c8bc5fa52b63b15b3ce26422d96ff70b8779\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built kiwipiepy_model sgmllib3k\n",
            "Installing collected packages: sgmllib3k, mmh3, kiwipiepy_model, types-requests, rfc3986, rank-bm25, pinecone-plugin-interface, pdf2image, ormsgpack, olefile, nh3, kiwipiepy, JPype1, feedparser, readme-renderer, pinecone-text, pinecone-client, pandas, konlpy, id, deepl, tavily-python, langgraph-sdk, anthropic, twine, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain-teddynote\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JPype1-1.6.0 anthropic-0.64.0 deepl-1.22.0 feedparser-6.0.11 id-1.5.0 kiwipiepy-0.21.0 kiwipiepy_model-0.21.0 konlpy-0.6.0 langchain-openai-0.3.30 langchain-teddynote-0.4.4 langgraph-0.6.5 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.0 mmh3-4.1.0 nh3-0.3.0 olefile-0.47 ormsgpack-1.10.0 pandas-2.3.1 pdf2image-1.17.0 pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7 pinecone-text-0.11.0 rank-bm25-0.2.2 readme-renderer-44.0 rfc3986-2.0.0 sgmllib3k-1.0.0 tavily-python-0.7.10 twine-6.1.0 types-requests-2.32.4.20250809\n",
            "LangSmith 추적을 시작합니다.\n",
            "[프로젝트명]\n",
            "CH10-Retriever\n"
          ]
        }
      ],
      "source": [
        "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
        "!pip install langchain-teddynote\n",
        "from langchain_teddynote import logging\n",
        "\n",
        "# 프로젝트 이름을 입력합니다.\n",
        "logging.langsmith(\"CH10-Retriever\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzwdS4OKqZEh",
        "outputId": "70530507-b183-4dd8-dc0a-d1a5d4a77b7e"
      },
      "id": "GzwdS4OKqZEh",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.14)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZKt1RkSrDqZ",
        "outputId": "31db9d8c-c6b9-4cb9-e2a7-783dd59b6ce9"
      },
      "id": "hZKt1RkSrDqZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee9b98ed",
      "metadata": {
        "id": "ee9b98ed"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# TextLoader를 사용하여 파일을 로드합니다.\n",
        "loader = TextLoader(\"/content/appendix-keywords.txt\")\n",
        "\n",
        "# 문서를 로드합니다.\n",
        "documents = loader.load()\n",
        "\n",
        "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "\n",
        "# 로드된 문서를 분할합니다.\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# OpenAI 임베딩을 생성합니다.\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
        "db = FAISS.from_documents(split_docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babc937f",
      "metadata": {
        "id": "babc937f"
      },
      "source": [
        "### VectorStore에서 VectorStoreRetriever 초기화(as_retriever)\n",
        "\n",
        "`as_retriever` 메서드는 VectorStore 객체를 기반으로 VectorStoreRetriever를 초기화하고 반환합니다. 이 메서드를 통해 다양한 검색 옵션을 설정하여 사용자의 요구에 맞는 문서 검색을 수행할 수 있습니다.\n",
        "\n",
        "**매개변수(parameters)**\n",
        "\n",
        "- `**kwargs`: 검색 함수에 전달할 키워드 인자\n",
        "  - `search_type`: 검색 유형 (\"similarity\", \"mmr\", \"similarity_score_threshold\")\n",
        "  - `search_kwargs`: 추가 검색 옵션\n",
        "    - `k`: 반환할 문서 수 (기본값: 4)\n",
        "    - `score_threshold`: similarity_score_threshold 검색의 최소 유사도 임계값\n",
        "    - `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
        "    - `lambda_mult`: MMR 결과의 다양성 조절 (0-1 사이, 기본값: 0.5)\n",
        "    - `filter`: 문서 메타데이터 기반 필터링\n",
        "\n",
        "**반환값(return)**\n",
        "\n",
        "- `VectorStoreRetriever`: 초기화된 VectorStoreRetriever 객체\n",
        "\n",
        "**참고**\n",
        "\n",
        "- 다양한 검색 전략 구현 가능 (유사도, MMR, 임계값 기반)\n",
        "- MMR (Maximal Marginal Relevance) 알고리즘으로 검색 결과의 다양성 조절 가능\n",
        "- 메타데이터 필터링으로 특정 조건의 문서만 검색 가능\n",
        "- `tags` 매개변수를 통해 검색기에 태그 추가 가능\n",
        "\n",
        "**주의사항**\n",
        "\n",
        "- `search_type`과 `search_kwargs`의 적절한 조합 필요\n",
        "- MMR 사용 시 `fetch_k`와 `k` 값의 균형 조절 필요\n",
        "- `score_threshold` 설정 시 너무 높은 값은 검색 결과가 없을 수 있음\n",
        "- 필터 사용 시 데이터셋의 메타데이터 구조 정확히 파악 필요\n",
        "- `lambda_mult` 값이 0에 가까울수록 다양성이 높아지고, 1에 가까울수록 유사성이 높아짐"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f79a365f",
      "metadata": {
        "id": "f79a365f"
      },
      "outputs": [],
      "source": [
        "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d741849",
      "metadata": {
        "id": "6d741849"
      },
      "source": [
        "### Retriever의 invoke()\n",
        "\n",
        "`invoke` 메서드는 Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용됩니다. 이 메서드는 동기적으로 Retriever를 호출하여 주어진 쿼리에 대한 관련 문서를 반환합니다.\n",
        "\n",
        "**매개변수(parameters)**\n",
        "\n",
        "- `input`: 검색 쿼리 문자열\n",
        "- `config`: Retriever 구성 (Optional[RunnableConfig])\n",
        "- `**kwargs`: Retriever에 전달할 추가 인자\n",
        "\n",
        "**반환값(return)**\n",
        "\n",
        "- `List[Document]`: 관련 문서 목록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8e39091",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8e39091",
        "outputId": "182bdbb9-db6f-43da-ce10-5bb7f4d204d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
            "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
            "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
            "\n",
            "Token\n",
            "=========================================================\n",
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "=========================================================\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "=========================================================\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# 관련 문서를 검색\n",
        "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
        "\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ab94c2c",
      "metadata": {
        "id": "6ab94c2c"
      },
      "source": [
        "### Max Marginal Relevance (MMR)\n",
        "\n",
        "`MMR(Maximal Marginal Relevance)` 방식은 쿼리에 대한 관련 항목을 검색할 때 검색된 문서의 **중복** 을 피하는 방법 중 하나입니다.\n",
        "\n",
        "단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 쿼리에 대한 **문서의 관련성** 과 이미 선택된 **문서들과의 차별성을 동시에 고려** 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f5c9cb",
      "metadata": {
        "id": "81f5c9cb"
      },
      "source": [
        "- `search_type` 매개변수를 `\"mmr\"` 로 설정하여 **MMR(Maximal Marginal Relevance)** 검색 알고리즘을 사용합니다.\n",
        "- `k`: 반환할 문서 수 (기본값: 4)\n",
        "- `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
        "- `lambda_mult`: MMR 결과의 다양성 조절 (0~1, 기본값: 0.5, 0: 유사도 점수만 고려, 1: 다양성만 고려)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8144a926",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8144a926",
        "outputId": "2f1ed244-c319-4244-8da7-849b00604afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
            "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
            "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
            "\n",
            "Token\n",
            "=========================================================\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# MMR(Maximal Marginal Relevance) 검색 유형을 지정\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6}\n",
        ")\n",
        "\n",
        "# 관련 문서를 검색합니다.\n",
        "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "902babfe",
      "metadata": {
        "id": "902babfe"
      },
      "source": [
        "### 유사도 점수 임계값 검색(similarity_score_threshold)\n",
        "\n",
        "유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법을 설정할 수 있습니다.\n",
        "\n",
        "임계값을 적절히 설정함으로써 **관련성이 낮은 문서를 필터링** 하고, 질의와 **가장 유사한 문서만 선별** 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476166f1",
      "metadata": {
        "id": "476166f1"
      },
      "source": [
        "- `search_type` 매개변수를 `\"similarity_score_threshold\"` 로 설정하여 유사도 점수 임계값을 기준으로 검색을 수행합니다.\n",
        "\n",
        "- `search_kwargs` 매개변수에 `{\"score_threshold\": 0.8}`를 전달하여 유사도 점수 임계값을 0.8로 설정합니다. 이는 검색 결과의 **유사도 점수가 0.8 이상인 문서만 반환됨** 을 의미합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b6509f52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6509f52",
        "outputId": "c2db724f-4789-4c7f-918f-d36c45c6a311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "retriever = db.as_retriever(\n",
        "    # 검색 유형을 \"similarity_score_threshold 으로 설정\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    # 임계값 설정\n",
        "    search_kwargs={\"score_threshold\": 0.8},\n",
        ")\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in retriever.invoke(\"Word2Vec 은 무엇인가요?\"):\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b16c14f",
      "metadata": {
        "id": "7b16c14f"
      },
      "source": [
        "### top_k 설정\n",
        "\n",
        "검색 시 사용할 `k` 와 같은 검색 키워드 인자(kwargs)를 지정할 수 있습니다.\n",
        "\n",
        "`k` 매개변수는 검색 결과에서 반환할 상위 결과의 개수를 나타냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be90b9b8",
      "metadata": {
        "id": "be90b9b8"
      },
      "source": [
        "- `search_kwargs`에서 `k` 매개변수를 1로 설정하여 검색 결과로 반환할 문서의 수를 지정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "081e0134",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "081e0134",
        "outputId": "aaf31785-0a9c-4932-e392-ad6db32c0b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
            "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
            "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
            "\n",
            "Token\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# k 설정\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "# 관련 문서를 검색\n",
        "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1168ca53",
      "metadata": {
        "id": "1168ca53"
      },
      "source": [
        "## 동적 설정(Configurable)\n",
        "\n",
        "- 검색 설정을 동적으로 조정하기 위해 `ConfigurableField` 를 사용합니다.\n",
        "- `ConfigurableField` 는 검색 매개변수의 고유 식별자, 이름, 설명을 설정하는 역할을 합니다.\n",
        "- 검색 설정을 조정하기 위해 `config` 매개변수를 사용하여 검색 설정을 지정합니다.\n",
        "- 검색 설정은 `config` 매개변수에 전달된 딕셔너리의 `configurable` 키에 저장됩니다.\n",
        "- 검색 설정은 검색 쿼리와 함께 전달되며, 검색 쿼리에 따라 동적으로 조정됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bc25029f",
      "metadata": {
        "id": "bc25029f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "# k 설정\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
        "    search_type=ConfigurableField(\n",
        "        id=\"search_type\",\n",
        "        name=\"Search Type\",\n",
        "        description=\"The search type to use\",\n",
        "    ),\n",
        "    search_kwargs=ConfigurableField(\n",
        "        # 검색 매개변수의 고유 식별자를 설정\n",
        "        id=\"search_kwargs\",\n",
        "        # 검색 매개변수의 이름을 설정\n",
        "        name=\"Search Kwargs\",\n",
        "        # 검색 매개변수에 대한 설명을 작성\n",
        "        description=\"The search kwargs to use\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721b750c",
      "metadata": {
        "id": "721b750c"
      },
      "source": [
        "아래는 동적 검색설정을 적용한 예시입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f2da4f32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2da4f32",
        "outputId": "911181b9-d5b9-4680-d306-b8674d662cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
            "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
            "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
            "\n",
            "Token\n",
            "=========================================================\n",
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "=========================================================\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# 검색 설정을 지정. Faiss 검색에서 k=3로 설정하여 가장 유사한 문서 3개를 반환\n",
        "config = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
        "\n",
        "# 관련 문서를 검색\n",
        "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\", config=config)\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "48f53c12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48f53c12",
        "outputId": "af8b7098-f720-4c40-e293-84b6c3fd144a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# 검색 설정을 지정. score_threshold 0.8 이상의 점수를 가진 문서만 반환\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"search_type\": \"similarity_score_threshold\",\n",
        "        \"search_kwargs\": {\n",
        "            \"score_threshold\": 0.8,\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "# 관련 문서를 검색\n",
        "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "951851c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951851c8",
        "outputId": "b26e1114-c86b-4942-9891-534229ea0f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "=========================================================\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "# 검색 설정을 지정. mmr 검색 설정.\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"search_type\": \"mmr\",\n",
        "        \"search_kwargs\": {\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6},\n",
        "    }\n",
        "}\n",
        "\n",
        "# 관련 문서를 검색\n",
        "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
        "\n",
        "# 관련 문서를 검색\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(\"=========================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbddf405",
      "metadata": {
        "id": "fbddf405"
      },
      "source": [
        "## Upstage 임베딩과 같이 Query & Passage embedding model 이 분리된 경우\n",
        "\n",
        "기본 retriever는 쿼리와 문서에 대해 동일한 임베딩 모델을 사용합니다.\n",
        "\n",
        "하지만 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용하는 경우가 있습니다.\n",
        "\n",
        "이러한 경우에는 쿼리 임베딩 모델을 사용하여 쿼리를 임베딩하고, 문서 임베딩 모델을 사용하여 문서를 임베딩합니다.\n",
        "\n",
        "이렇게 하면 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-upstage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzNENsk9rTNa",
        "outputId": "c2222ac8-84e4-4317-8364-664881aa7044"
      },
      "id": "HzNENsk9rTNa",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-upstage\n",
            "  Downloading langchain_upstage-0.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-upstage) (0.3.74)\n",
            "Requirement already satisfied: langchain-openai<0.4,>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain-upstage) (0.3.30)\n",
            "Collecting pypdf<5.0.0,>=4.2.0 (from langchain-upstage)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from langchain-upstage) (2.32.3)\n",
            "Collecting tokenizers<0.21.0,>=0.20.0 (from langchain-upstage)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-upstage) (2.11.7)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4,>=0.3->langchain-upstage) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<0.4,>=0.3->langchain-upstage) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2025.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<0.21.0,>=0.20.0->langchain-upstage) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.21.0,>=0.20.0->langchain-upstage) (1.1.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai<0.4,>=0.3->langchain-upstage) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai<0.4,>=0.3->langchain-upstage) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai<0.4,>=0.3->langchain-upstage) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai<0.4,>=0.3->langchain-upstage) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<0.4,>=0.3->langchain-upstage) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.29->langchain-upstage) (0.16.0)\n",
            "Downloading langchain_upstage-0.7.1-py3-none-any.whl (20 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, tokenizers, langchain-upstage\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.55.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-upstage-0.7.1 pypdf-4.3.1 tokenizers-0.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = \"upstage API\""
      ],
      "metadata": {
        "id": "LdIMcV2itD-A"
      },
      "id": "LdIMcV2itD-A",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6b51093b",
      "metadata": {
        "id": "6b51093b"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "\n",
        "# TextLoader를 사용하여 파일을 로드합니다.\n",
        "loader = TextLoader(\"/content/appendix-keywords.txt\")\n",
        "\n",
        "# 문서를 로드합니다.\n",
        "documents = loader.load()\n",
        "\n",
        "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "\n",
        "# 로드된 문서를 분할합니다.\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Upstage 임베딩을 생성합니다. 문서용 모델을 사용합니다.\n",
        "doc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
        "\n",
        "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
        "db = FAISS.from_documents(split_docs, doc_embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424f69e7",
      "metadata": {
        "id": "424f69e7"
      },
      "source": [
        "아래는 쿼리용 Upstage 임베딩을 생성하고, 쿼리 문장을 벡터로 변환하여 벡터 유사도 검색을 수행하는 예시입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "aa46e289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa46e289",
        "outputId": "061438ab-a6b2-4422-cc88-9572fd69523f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='1c977f36-7661-4359-abb5-069a07ddaf18', metadata={'source': '/content/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken'),\n",
              " Document(id='d23c354c-f413-4079-9e6f-081bdcdfb316', metadata={'source': '/content/appendix-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# 쿼리용 Upstage 임베딩을 생성합니다. 쿼리용 모델을 사용합니다.\n",
        "query_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
        "\n",
        "# 쿼리 문장을 벡터로 변환합니다.\n",
        "query_vector = query_embedder.embed_query(\"임베딩(Embedding)은 무엇인가요?\")\n",
        "\n",
        "# 벡터 유사도 검색을 수행하여 가장 유사한 2개의 문서를 반환합니다.\n",
        "db.similarity_search_by_vector(query_vector, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5BVxXfGsuXzX"
      },
      "id": "5BVxXfGsuXzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNMn9TFbuXm2"
      },
      "id": "ZNMn9TFbuXm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문맥 압축 검색기(ContextualCompressionRetriever)\n",
        "\n",
        "검색 시스템에서 직면하는 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 특정 질의를 처리해야 할지 미리 알 수 없다는 점입니다.\n",
        "\n",
        "이는 질의와 가장 관련성이 높은 정보가 많은 양의 무관한 텍스트를 포함한 문서에 묻혀 있을 수 있음을 의미합니다.\n",
        "\n",
        "이러한 전체 문서를 애플리케이션에 전달하면 더 비용이 많이 드는 LLM 호출과 품질이 낮은 응답으로 이어질 수 있습니다.\n",
        "\n",
        "`ContextualCompressionRetriever` 은 이 문제를 해결하기 위해 고안되었습니다.\n",
        "\n",
        "아이디어는 간단합니다. 검색된 문서를 그대로 즉시 반환하는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환되도록 할 수 있습니다.\n",
        "\n",
        "여기서 \"압축\"은 개별 문서의 내용을 압축하는 것과 문서를 전체적으로 필터링하는 것 모두를 의미합니다.\n",
        "\n",
        "`ContextualCompressionRetriever` 는 질의를 base retriever에 전달하고, 초기 문서를 가져와 Document Compressor를 통과시킵니다.\n",
        "\n",
        "Document Compressor는 문서 목록을 가져와 문서의 내용을 줄이거나 문서를 완전히 삭제하여 목록을 축소합니다.\n",
        "\n",
        "> 출처: https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v\n",
        "\n",
        "![](./images/01-Contextual-Compression.jpeg)\n"
      ],
      "metadata": {
        "id": "dNVEakOCvJ6V"
      },
      "id": "dNVEakOCvJ6V"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3fd1078f"
      },
      "outputs": [],
      "source": [
        "# 패키지 업데이트\n",
        "!pip install -qU langchain-teddynote"
      ],
      "id": "3fd1078f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fca927c"
      },
      "source": [
        "`pretty_print_docs` 함수는 문서 리스트를 예쁘게 출력하는 헬퍼 함수입니다.\n"
      ],
      "id": "9fca927c"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "826a3625"
      },
      "outputs": [],
      "source": [
        "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ],
      "id": "826a3625"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef9cb273"
      },
      "source": [
        "## 기본 Retriever 설정\n",
        "\n",
        "간단한 벡터 스토어 retriever를 초기화하고 텍스트 문서를 청크 단위로 저장하는 것부터 시작해 보겠습니다.\n",
        "\n",
        "예시 질문을 던졌을 때, retriever는 관련 있는 문서 1~2개와 관련 없는 문서 몇 개를 반환하는 것을 확인할 수 있습니다."
      ],
      "id": "ef9cb273"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "066500cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cde60ea-f22a-442a-9279-ae652c9bad77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
            "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
            "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
            "\n",
            "Page Rank\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
            "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
            "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
            "\n",
            "데이터 마이닝\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# TextLoader를 사용하여 \"appendix-keywords.txt\" 파일에서 문서를 로드합니다.\n",
        "loader = TextLoader(\"/content/appendix-keywords.txt\")\n",
        "\n",
        "# CharacterTextSplitter를 사용하여 문서를 청크 크기 300자와 청크 간 중복 0으로 분할합니다.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "texts = loader.load_and_split(text_splitter)\n",
        "\n",
        "# OpenAIEmbeddings를 사용하여 FAISS 벡터 저장소를 생성하고 검색기로 변환합니다.\n",
        "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
        "\n",
        "# 쿼리에 질문을 정의하고 관련 문서를 검색합니다.\n",
        "docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
        "\n",
        "# 검색된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(docs)"
      ],
      "id": "066500cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c77a320"
      },
      "source": [
        "## 맥락적 압축(ContextualCompression)\n",
        "\n",
        "`LLMChainExtractor` 를 활용하여 생성한 `DocumentCompressor` 를 retriever 에 적용한 것이 바로 `ContextualCompressionRetriever` 입니다."
      ],
      "id": "6c77a320"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b5464a6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa39a9b1-be48-4343-a41d-82a3ace80a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
            "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
            "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
            "\n",
            "Page Rank\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
            "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
            "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
            "\n",
            "데이터 마이닝\n",
            "=========================================================\n",
            "============== LLMChainExtractor 적용 후 ==================\n",
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  # OpenAI 언어 모델 초기화\n",
        "\n",
        "# LLM을 사용하여 문서 압축기 생성\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "pretty_print_docs(retriever.invoke(\"Semantic Search 에 대해서 알려줘.\"))\n",
        "\n",
        "print(\"=========================================================\")\n",
        "print(\"============== LLMChainExtractor 적용 후 ==================\")\n",
        "\n",
        "compressed_docs = (\n",
        "    compression_retriever.invoke(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n",
        "        \"Semantic Search 에 대해서 알려줘.\"\n",
        "    )\n",
        ")\n",
        "pretty_print_docs(compressed_docs)  # 검색된 문서를 예쁘게 출력"
      ],
      "id": "b5464a6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35595f3"
      },
      "source": [
        "## LLM 을 활용한 문서 필터링\n",
        "\n",
        "### `LLMChainFilter`\n",
        "\n",
        "`LLMChainFilter`는 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반환할지 결정하기 위해 LLM 체인을 사용하는 보다 단순하지만 강력한 압축기입니다.\n",
        "\n",
        "이 필터는 문서 내용을 변경(압축)하지 않고 문서를 **선택적으로 반환** 합니다.\n"
      ],
      "id": "f35595f3"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "59ec2a95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cf5e35-407d-4044-8800-1f5f15b41b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n"
          ]
        }
      ],
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainFilter\n",
        "\n",
        "# LLM을 사용하여 LLMChainFilter 객체를 생성합니다.\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
        "    base_compressor=_filter,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)  # 압축된 문서를 예쁘게 출력합니다."
      ],
      "id": "59ec2a95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a8890e0"
      },
      "source": [
        "### `EmbeddingsFilter`\n",
        "\n",
        "각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느립니다.\n",
        "\n",
        "`EmbeddingsFilter`는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠른 옵션을 제공합니다.\n",
        "\n",
        "이를 통해 검색 결과의 관련성을 유지하면서도 계산 비용과 시간을 절약할 수 있습니다.\n"
      ],
      "id": "8a8890e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb9e39f1"
      },
      "source": [
        "`EmbeddingsFilter` 와 `ContextualCompressionRetriever` 를 사용하여 관련 문서를 압축하고 검색하는 과정입니다.\n",
        "\n",
        "- `EmbeddingsFilter` 를 사용하여 지정된 **유사도 임계값(0.86)** 이상인 문서를 필터링 합니다."
      ],
      "id": "cb9e39f1"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "f30565ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d3dd12-18b4-4d0d-e627-3f47932b15d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 유사도 임계값이 0.76인 EmbeddingsFilter 객체를 생성합니다.\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n",
        "\n",
        "# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성합니다.\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter, base_retriever=retriever\n",
        ")\n",
        "\n",
        "# ContextualCompressionRetriever 객체를 사용하여 관련 문서를 검색합니다.\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "# 압축된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "id": "f30565ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db1d1b3"
      },
      "source": [
        "## 파이프라인 생성(압축기+문서 변환기)\n",
        "\n",
        "`DocumentCompressorPipeline` 을 사용하면 여러 compressor를 순차적으로 결합할 수 있습니다.\n",
        "\n",
        "Compressor와 함께 `BaseDocumentTransformer`를 파이프라인에 추가할 수 있는데, 이는 맥락적 압축을 수행하지 않고 단순히 문서 집합에 대한 변환을 수행합니다.\n",
        "\n",
        "예를 들어, `TextSplitter`는 문서를 더 작은 조각으로 분할하기 위해 document transformer로 사용될 수 있으며, `EmbeddingsRedundantFilter`는 문서 간의 임베딩 유사성(기본값: 0.95 유사도 이상을 중복 문서로 간주) 을 기반으로 중복 문서를 필터링하는 데 사용될 수 있습니다.\n",
        "\n",
        "아래에서는 먼저 문서를 더 작은 청크로 분할한 다음, 중복 문서를 제거하고, 쿼리와의 관련성을 기준으로 필터링하여 compressor pipeline을 생성합니다.\n"
      ],
      "id": "9db1d1b3"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "a00fde5a"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정합니다.\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "\n",
        "# 임베딩을 사용하여 중복 필터를 생성합니다.\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "\n",
        "# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정합니다.\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n",
        "\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    # 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터, LLMChainExtractor를 변환기로 설정합니다.\n",
        "    transformers=[\n",
        "        splitter,\n",
        "        redundant_filter,\n",
        "        relevant_filter,\n",
        "        LLMChainExtractor.from_llm(llm),\n",
        "    ]\n",
        ")"
      ],
      "id": "a00fde5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd14e247"
      },
      "source": [
        "`ContextualCompressionRetriever`를 초기화하며, `base_compressor`로 `pipeline_compressor`를, `base_retriever`로 `retriever`를 사용합니다.\n"
      ],
      "id": "bd14e247"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "022626dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4829ddd4-1900-4cd7-bc3c-3c772fecb850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
          ]
        }
      ],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화합니다.\n",
        "    base_compressor=pipeline_compressor,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "# 압축된 문서를 예쁘게 출력합니다.\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "id": "022626dd"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhGsD5MLu8Av"
      },
      "id": "vhGsD5MLu8Av",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldaqXJiau7-B"
      },
      "id": "ldaqXJiau7-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7904228"
      },
      "source": [
        "# 앙상블 검색기(Ensemble Retriever)\n",
        "\n",
        "`EnsembleRetriever`는 여러 검색기를 결합하여 더 강력한 검색 결과를 제공하는 LangChain의 기능입니다. 이 검색기는 다양한 검색 알고리즘의 장점을 활용하여 단일 알고리즘보다 더 나은 성능을 달성할 수 있습니다.\n",
        "\n",
        "**주요 특징**\n",
        "1. 여러 검색기 통합: 다양한 유형의 검색기를 입력으로 받아 결과를 결합합니다.\n",
        "2. 결과 재순위화: [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) 알고리즘을 사용하여 결과의 순위를 조정합니다.\n",
        "3. 하이브리드 검색: 주로 `sparse retriever`(예: BM25)와 `dense retriever`(예: 임베딩 유사도)를 결합하여 사용합니다.\n",
        "\n",
        "**장점**\n",
        "- Sparse retriever: 키워드 기반 검색에 효과적\n",
        "- Dense retriever: 의미적 유사성 기반 검색에 효과적\n",
        "\n",
        "이러한 상호 보완적인 특성으로 인해 `EnsembleRetriever`는 다양한 검색 시나리오에서 향상된 성능을 제공할 수 있습니다.\n",
        "\n",
        "자세한 내용은 [LangChain 공식 문서](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)를 참조하세요.\n"
      ],
      "id": "b7904228"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ff8adf"
      },
      "source": [
        "- `EnsembleRetriever`를 초기화하여 `BM25Retriever`와 `FAISS` 검색기를 결합합니다. 각 검색기의 가중치를 설정됩니다.\n"
      ],
      "id": "45ff8adf"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3f2b299c"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 샘플 문서 리스트\n",
        "doc_list = [\n",
        "    \"I like apples\",\n",
        "    \"I like apple company\",\n",
        "    \"I like apple's iphone\",\n",
        "    \"Apple is my favorite company\",\n",
        "    \"I like apple's ipad\",\n",
        "    \"I like apple's macbook\",\n",
        "]\n",
        "\n",
        "\n",
        "# bm25 retriever와 faiss retriever를 초기화합니다.\n",
        "bm25_retriever = BM25Retriever.from_texts(\n",
        "    doc_list,\n",
        ")\n",
        "bm25_retriever.k = 1  # BM25Retriever의 검색 결과 개수를 1로 설정합니다.\n",
        "\n",
        "embedding = OpenAIEmbeddings()  # OpenAI 임베딩을 사용합니다.\n",
        "faiss_vectorstore = FAISS.from_texts(\n",
        "    doc_list,\n",
        "    embedding,\n",
        ")\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "# 앙상블 retriever를 초기화합니다.\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        "    weights=[0.7, 0.3],\n",
        ")"
      ],
      "id": "3f2b299c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "835eb07f"
      },
      "source": [
        "`ensemble_retriever` 객체의 `get_relevant_documents()` 메서드를 호출하여 관련성 높은 문서를 검색합니다.\n"
      ],
      "id": "835eb07f"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cfacdd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037b5e4f-9dc1-4fb0-bde6-f5998da5d983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ensemble Retriever]\n",
            "Content: Apple is my favorite company\n",
            "\n",
            "Content: I like apples\n",
            "\n",
            "[BM25 Retriever]\n",
            "Content: Apple is my favorite company\n",
            "\n",
            "[FAISS Retriever]\n",
            "Content: I like apples\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 검색 결과 문서를 가져옵니다.\n",
        "query = \"my favorite fruit is apple\"\n",
        "ensemble_result = ensemble_retriever.invoke(query)\n",
        "bm25_result = bm25_retriever.invoke(query)\n",
        "faiss_result = faiss_retriever.invoke(query)\n",
        "\n",
        "# 가져온 문서를 출력합니다.\n",
        "print(\"[Ensemble Retriever]\")\n",
        "for doc in ensemble_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()\n",
        "\n",
        "print(\"[BM25 Retriever]\")\n",
        "for doc in bm25_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()\n",
        "\n",
        "print(\"[FAISS Retriever]\")\n",
        "for doc in faiss_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()"
      ],
      "id": "cfacdd8d"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dac4523b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b601b42-6fef-4995-9471-5f457e18c7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ensemble Retriever]\n",
            "Content: Apple is my favorite company\n",
            "\n",
            "Content: I like apple's iphone\n",
            "\n",
            "[BM25 Retriever]\n",
            "Content: Apple is my favorite company\n",
            "\n",
            "[FAISS Retriever]\n",
            "Content: I like apple's iphone\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 검색 결과 문서를 가져옵니다.\n",
        "query = \"Apple company makes my favorite iphone\"\n",
        "ensemble_result = ensemble_retriever.invoke(query)\n",
        "bm25_result = bm25_retriever.invoke(query)\n",
        "faiss_result = faiss_retriever.invoke(query)\n",
        "\n",
        "# 가져온 문서를 출력합니다.\n",
        "print(\"[Ensemble Retriever]\")\n",
        "for doc in ensemble_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()\n",
        "\n",
        "print(\"[BM25 Retriever]\")\n",
        "for doc in bm25_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()\n",
        "\n",
        "print(\"[FAISS Retriever]\")\n",
        "for doc in faiss_result:\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print()"
      ],
      "id": "dac4523b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9d4a5a2"
      },
      "source": [
        "## 런타임 Config 변경\n",
        "\n",
        "런타임에서도 retriever 의 속성을 변경할 수 있습니다. 이는 `ConfigurableField` 클래스를 사용하여 가능합니다."
      ],
      "id": "c9d4a5a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae1c63e"
      },
      "source": [
        "- `weights` 매개변수를 `ConfigurableField` 객체로 정의합니다.\n",
        "  - 필드의 ID는 \"ensemble_weights\"로 설정합니다.\n"
      ],
      "id": "aae1c63e"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "84a34e4b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    # 리트리버 목록을 설정합니다. 여기서는 bm25_retriever와 faiss_retriever를 사용합니다.\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        ").configurable_fields(\n",
        "    weights=ConfigurableField(\n",
        "        # 검색 매개변수의 고유 식별자를 설정합니다.\n",
        "        id=\"ensemble_weights\",\n",
        "        # 검색 매개변수의 이름을 설정합니다.\n",
        "        name=\"Ensemble Weights\",\n",
        "        # 검색 매개변수에 대한 설명을 작성합니다.\n",
        "        description=\"Ensemble Weights\",\n",
        "    )\n",
        ")"
      ],
      "id": "84a34e4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0e4fb1"
      },
      "source": [
        "- 검색 시 `config` 매개변수를 통해 검색 설정을 지정합니다.\n",
        "  - `ensemble_weights` 옵션의 가중치를 [1, 0]으로 설정하여 **모든 검색 결과의 가중치가 BM25 retriever 에 더 많이 부여** 되도록 합니다."
      ],
      "id": "bb0e4fb1"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1bee1000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65f2394-326a-41cf-83c2-8ef0f12406e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Apple is my favorite company'),\n",
              " Document(id='87cd67d4-e126-45d4-9d4c-9987554669e4', metadata={}, page_content='I like apples')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "config = {\"configurable\": {\"ensemble_weights\": [1, 0]}}\n",
        "\n",
        "# config 매개변수를 사용하여 검색 설정을 지정합니다.\n",
        "docs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n",
        "docs  # 검색 결과인 docs를 출력합니다."
      ],
      "id": "1bee1000"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffdad0be"
      },
      "source": [
        "이번에는 검색시 모든 검색 결과의 가중치가 **FAISS retriever 에 더 많이 부여** 되도록 합니다."
      ],
      "id": "ffdad0be"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5d95922b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913430b2-d388-4bc4-e760-838f3e63c80d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='87cd67d4-e126-45d4-9d4c-9987554669e4', metadata={}, page_content='I like apples'),\n",
              " Document(metadata={}, page_content='Apple is my favorite company')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "config = {\"configurable\": {\"ensemble_weights\": [0, 1]}}\n",
        "\n",
        "# config 매개변수를 사용하여 검색 설정을 지정합니다.\n",
        "docs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n",
        "docs  # 검색 결과인 docs를 출력합니다."
      ],
      "id": "5d95922b"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NpCTjaXvZ9Z"
      },
      "id": "5NpCTjaXvZ9Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hohr-0WcvZ6p"
      },
      "id": "Hohr-0WcvZ6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae69c074"
      },
      "source": [
        "# 긴 문맥 재정렬(LongContextReorder)\n",
        "\n",
        "모델의 아키텍처와 상관없이, 10개 이상의 검색된 문서를 포함할 경우 성능이 상당히 저하됩니다.\n",
        "\n",
        "간단히 말해, 모델이 긴 컨텍스트 중간에 있는 관련 정보에 접근해야 할 때, 제공된 문서를 무시하는 경향이 있습니다.\n",
        "\n",
        "자세한 내용은 다음 논문을 참조하세요\n",
        "\n",
        "- https://arxiv.org/abs/2307.03172\n",
        "\n",
        "이 문제를 피하기 위해, 검색 후 문서의 순서를 재배열하여 성능 저하를 방지할 수 있습니다.\n"
      ],
      "id": "ae69c074"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9283f84e"
      },
      "source": [
        "- `Chroma` 벡터 저장소를 사용하여 텍스트 데이터를 저장하고 검색할 수 있는 `retriever`를 생성합니다.\n",
        "- `retriever`의 `invoke` 메서드를 사용하여 주어진 쿼리에 대해 관련성이 높은 문서를 검색합니다.\n"
      ],
      "id": "9283f84e"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8S3Iw83vk7M",
        "outputId": "36f02ccf-540c-439c-f595-d47c7a4c1146"
      },
      "id": "z8S3Iw83vk7M",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.17-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.17-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=7328476b623cdee95365c3e682862dfd9ce512a03caa2d008c3059595fee4c28\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.17 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "089f999e"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "# 임베딩을 가져옵니다.\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "texts = [\n",
        "    \"이건 그냥 내가 아무렇게나 적어본 글입니다.\",\n",
        "    \"사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\",\n",
        "    \"아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\",\n",
        "    \"챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\",\n",
        "    \"챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\",\n",
        "    \"애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\",\n",
        "    \"ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\",\n",
        "    \"비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\",\n",
        "    \"ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\",\n",
        "    \"FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\",\n",
        "]\n",
        "\n",
        "\n",
        "# 검색기를 생성합니다. (K는 10으로 설정합니다)\n",
        "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
        "    search_kwargs={\"k\": 10}\n",
        ")"
      ],
      "id": "089f999e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8887d361"
      },
      "source": [
        "검색기에 쿼리를 입력하여 검색을 수행합니다.\n"
      ],
      "id": "8887d361"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2871aeaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a1161d-c93d-4406-b47d-d43be3929bf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'),\n",
              " Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'),\n",
              " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
              " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
              " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
              " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
              " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
              " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
              " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
              " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "query = \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\"\n",
        "\n",
        "# 관련성 점수에 따라 정렬된 관련 문서를 가져옵니다.\n",
        "docs = retriever.invoke(query)\n",
        "docs"
      ],
      "id": "2871aeaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b50f6b95"
      },
      "source": [
        "`LongContextReorder` 클래스의 인스턴스인 `reordering`을 생성합니다.\n",
        "\n",
        "- `reordering.transform_documents(docs)`를 호출하여 문서 목록 `docs`를 재정렬합니다.\n",
        "  - 덜 관련된 문서는 목록의 중간에 위치하고, 더 관련된 문서는 시작과 끝에 위치하도록 재정렬됩니다.\n"
      ],
      "id": "b50f6b95"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ae93a9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aadc1a0-6df3-4c65-e748-4ab3a0d12728"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'),\n",
              " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
              " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
              " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
              " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.'),\n",
              " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
              " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
              " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
              " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
              " Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# 문서를 재정렬합니다\n",
        "# 덜 관련된 문서는 목록의 중간에 위치하고 더 관련된 요소는 시작/끝에 위치합니다.\n",
        "reordering = LongContextReorder()\n",
        "reordered_docs = reordering.transform_documents(docs)\n",
        "\n",
        "# 4개의 관련 문서가 시작과 끝에 위치하는지 확인합니다.\n",
        "reordered_docs"
      ],
      "id": "ae93a9a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6292e32"
      },
      "source": [
        "## Context Reordering을 사용하여 질의-응답 체인 생성\n"
      ],
      "id": "c6292e32"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1f2ea0a4"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\".join([doc.page_content for i, doc in enumerate(docs)])"
      ],
      "id": "1f2ea0a4"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4a645502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfd2fc7-0760-4111-f158-be09e97cb7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\n",
            "ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\n",
            "사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\n",
            "챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\n",
            "이건 그냥 내가 아무렇게나 적어본 글입니다.\n",
            "챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\n",
            "비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\n",
            "아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\n",
            "애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\n",
            "FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\n"
          ]
        }
      ],
      "source": [
        "print(format_docs(docs))"
      ],
      "id": "4a645502"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2c9b07e1"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\".join(\n",
        "        [\n",
        "            f\"[{i}] {doc.page_content} [source: teddylee777@gmail.com]\"\n",
        "            for i, doc in enumerate(docs)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def reorder_documents(docs):\n",
        "    # 재정렬\n",
        "    reordering = LongContextReorder()\n",
        "    reordered_docs = reordering.transform_documents(docs)\n",
        "    combined = format_docs(reordered_docs)\n",
        "    print(combined)\n",
        "    return combined"
      ],
      "id": "2c9b07e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9af013ad"
      },
      "source": [
        "재정렬된 문서를 출력합니다.\n"
      ],
      "id": "9af013ad"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "a193bc14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20dbfb04-5512-4dad-b9ca-39b4c24cd64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]\n",
            "[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]\n",
            "[2] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]\n",
            "[3] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]\n",
            "[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]\n",
            "[5] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]\n",
            "[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]\n",
            "[7] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]\n",
            "[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]\n",
            "[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]\n"
          ]
        }
      ],
      "source": [
        "# 재정렬된 문서를 출력\n",
        "_ = reorder_documents(docs)"
      ],
      "id": "a193bc14"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "703afe03"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "template = \"\"\"Given this text extracts:\n",
        "{context}\n",
        "\n",
        "-----\n",
        "Please answer the following question:\n",
        "{question}\n",
        "\n",
        "Answer in the following languages: {language}\n",
        "\"\"\"\n",
        "\n",
        "# 프롬프트 정의\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Chain 정의\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\")\n",
        "        | retriever\n",
        "        | RunnableLambda(reorder_documents),  # 질문을 기반으로 문맥을 검색합니다.\n",
        "        \"question\": itemgetter(\"question\"),  # 질문을 추출합니다.\n",
        "        \"language\": itemgetter(\"language\"),  # 답변 언어를 추출합니다.\n",
        "    }\n",
        "    | prompt  # 프롬프트 템플릿에 값을 전달합니다.\n",
        "    | ChatOpenAI(model=\"gpt-4o-mini\")  # 언어 모델에 프롬프트를 전달합니다.\n",
        "    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n",
        ")"
      ],
      "id": "703afe03"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89f08a63"
      },
      "source": [
        "`question` 에 쿼리를 입력하고 `language` 에 언어를 입력합니다.\n",
        "\n",
        "- 재정렬된 문서의 검색 결과도 확인합니다.\n"
      ],
      "id": "89f08a63"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9249067c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51abed8-01c7-4c38-8368-1af1cc834e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다. [source: teddylee777@gmail.com]\n",
            "[1] 챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다. [source: teddylee777@gmail.com]\n",
            "[2] 챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. [source: teddylee777@gmail.com]\n",
            "[3] 아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다. [source: teddylee777@gmail.com]\n",
            "[4] FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다. [source: teddylee777@gmail.com]\n",
            "[5] 애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다. [source: teddylee777@gmail.com]\n",
            "[6] 비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다. [source: teddylee777@gmail.com]\n",
            "[7] 이건 그냥 내가 아무렇게나 적어본 글입니다. [source: teddylee777@gmail.com]\n",
            "[8] 사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다. [source: teddylee777@gmail.com]\n",
            "[9] ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다. [source: teddylee777@gmail.com]\n"
          ]
        }
      ],
      "source": [
        "answer = chain.invoke(\n",
        "    {\"question\": \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\", \"language\": \"KOREAN\"}\n",
        ")"
      ],
      "id": "9249067c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f24418a"
      },
      "source": [
        "답변을 출력합니다.\n"
      ],
      "id": "1f24418a"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "2ef4b288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b07a6a-13af-440a-ed26-b851c2606c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT는 OpenAI에 의해 개발된 인공지능 모델로, 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다. 이 모델은 지속적으로 개선되고 있으며, 다양한 질문에 답하거나 복잡한 문제를 해결하는 데에도 사용될 수 있습니다. 사용자와 대화하는 형태로 설계되어 있어, 자연스럽고 유용한 대화를 나눌 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "print(answer)"
      ],
      "id": "2ef4b288"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CooXTDuKvysX"
      },
      "id": "CooXTDuKvysX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9h-sZgCfvykZ"
      },
      "id": "9h-sZgCfvykZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174ef1d9"
      },
      "source": [
        "# Parent Document Retriever\n",
        "\n",
        "**문서 검색과 문서 분할의 균형 잡기**\n",
        "\n",
        "문서 검색 과정에서 문서를 적절한 크기의 조각(청크)으로 나누는 것은 다음의 **상충되는 두 가지 중요한 요소를 고려**해야 합니다.\n",
        "\n",
        "1. 작은 문서를 원하는 경우: 이렇게 하면 문서의 임베딩이 그 의미를 가장 정확하게 반영할 수 있습니다. 문서가 너무 길면 임베딩이 의미를 잃어버릴 수 있습니다.\n",
        "2. 각 청크의 맥락이 유지되도록 충분히 긴 문서를 원하는 경우입니다.\n",
        "\n",
        "**`ParentDocumentRetriever`의 역할**\n",
        "\n",
        "이 두 요구 사항 사이의 균형을 맞추기 위해 `ParentDocumentRetriever`라는 도구가 사용됩니다. 이 도구는 문서를 작은 조각으로 나누고, 이 조각들을 관리합니다. 검색을 진행할 때는, 먼저 이 작은 조각들을 찾아낸 다음, 이 조각들이 속한 원본 문서(또는 더 큰 조각)의 식별자(ID)를 통해 전체적인 맥락을 파악할 수 있습니다.\n",
        "\n",
        "여기서 '부모 문서'란, 작은 조각이 나누어진 원본 문서를 말합니다. 이는 전체 문서일 수도 있고, 비교적 큰 다른 조각일 수도 있습니다. 이 방식을 통해 문서의 의미를 정확하게 파악하면서도, 전체적인 맥락을 유지할 수 있게 됩니다.\n",
        "\n",
        "**정리**\n",
        "\n",
        "- **문서 간의 계층 구조 활용**: `ParentDocumentRetriever`는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용합니다.\n",
        "- **검색 성능 향상**: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 찾아낼 수 있습니다.\n",
        "  문서를 검색할 때 자주 발생하는 두 가지 상충되는 요구 사항이 있습니다:\n"
      ],
      "id": "174ef1d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ba411b"
      },
      "source": [
        "여러 개의 텍스트 파일을 로드하기 위해 `TextLoader` 객체를 생성하고 데이터를 로드합니다.\n"
      ],
      "id": "d7ba411b"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCzuxU9wv9T9",
        "outputId": "a05817c3-8d86-40b5-de75-2cb79dd8c76b"
      },
      "id": "HCzuxU9wv9T9",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.74)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.0.17)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.20.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.25.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain_chroma) (0.4.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain_chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain_chroma) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain_chroma) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: langchain_chroma\n",
            "Successfully installed langchain_chroma-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9c12ba1e"
      },
      "outputs": [],
      "source": [
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever"
      ],
      "id": "9c12ba1e"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "68f6a054"
      },
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    # 파일을 로드합니다.\n",
        "    TextLoader(\"/content/appendix-keywords.txt\"),\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    # 로더를 사용하여 문서를 로드하고 docs 리스트에 추가합니다.\n",
        "    docs.extend(loader.load())"
      ],
      "id": "68f6a054"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5314377a"
      },
      "source": [
        "## 전체 문서 검색\n",
        "\n",
        "이 모드에서는 전체 문서를 검색하고자 합니다. 따라서 `child_splitter` 만 지정하도록 하겠습니다.\n",
        "\n",
        "- 나중에는 `parent_splitter` 도 지정하여 결과를 비교해 보겠습니다.\n"
      ],
      "id": "5314377a"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "812dfdfe"
      },
      "outputs": [],
      "source": [
        "# 자식 분할기를 생성합니다.\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "# DB를 생성합니다.\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "# Retriever 를 생성합니다.\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ],
      "id": "812dfdfe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02d1967f"
      },
      "source": [
        "`retriever.add_documents(docs, ids=None)` 함수로 문서목록을 추가합니다.\n",
        "\n",
        "- `ids` 가 `None` 이면 자동으로 생성됩니다.\n",
        "- `add_to_docstore=False` 로 설정시 document 를 중복으로 추가하지 않습니다. 단, 중복을 체크하기 위한 `ids` 값이 필수 값으로 요구됩니다.\n"
      ],
      "id": "02d1967f"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "c737c91d"
      },
      "outputs": [],
      "source": [
        "# 문서를 검색기에 추가합니다. docs는 문서 목록이고, ids는 문서의 고유 식별자 목록입니다.\n",
        "retriever.add_documents(docs, ids=None, add_to_docstore=True)"
      ],
      "id": "c737c91d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ef1793"
      },
      "source": [
        "이 코드는 두 개의 키를 반환해야 합니다. 그 이유는 우리가 두 개의 문서를 추가했기 때문입니다.\n"
      ],
      "id": "68ef1793"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72bc0f5b"
      },
      "source": [
        "- `store` 객체의 `yield_keys()` 메서드를 호출하여 반환된 키(key) 값들을 리스트로 변환합니다.\n"
      ],
      "id": "72bc0f5b"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6875b63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb47125-c7e2-4201-e79d-1df51c9f6cab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c6463b1a-2954-4da2-9ead-83c3d9101e88']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# 저장소의 모든 키를 리스트로 반환합니다.\n",
        "list(store.yield_keys())"
      ],
      "id": "6875b63a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb5f346"
      },
      "source": [
        "이제 벡터 스토어 검색 기능을 호출해 보겠습니다.\n",
        "\n",
        "우리가 작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환되는 것을 확인할 수 있을 것입니다.\n"
      ],
      "id": "9fb5f346"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c15a995"
      },
      "source": [
        "`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n"
      ],
      "id": "1c15a995"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "d6ed9a29"
      },
      "outputs": [],
      "source": [
        "# 유사도 검색을 수행합니다.\n",
        "sub_docs = vectorstore.similarity_search(\"Word2Vec\")"
      ],
      "id": "d6ed9a29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f377d0d9"
      },
      "source": [
        "`sub_docs[0].page_content`를 출력합니다.\n"
      ],
      "id": "f377d0d9"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "4db52fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6963add-f85b-48b8-8ae4-81d1b7fc9092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n"
          ]
        }
      ],
      "source": [
        "# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\n",
        "print(sub_docs[0].page_content)"
      ],
      "id": "4db52fa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399e28a3"
      },
      "source": [
        "이제 전체 retriever에서 검색해 보겠습니다. 이 과정에서는 작은 청크(chunk)들이 위치한 **문서를 반환** 하기 때문에 상대적으로 큰 문서들이 반환될 것입니다.\n"
      ],
      "id": "399e28a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f21c8dce"
      },
      "source": [
        "`retriever` 객체의 `invoke()` 메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.\n"
      ],
      "id": "f21c8dce"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bd845482"
      },
      "outputs": [],
      "source": [
        "# 문서를 검색하여 가져옵니다.\n",
        "retrieved_docs = retriever.invoke(\"Word2Vec\")"
      ],
      "id": "bd845482"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a3cb5b8"
      },
      "source": [
        "검색된 문서(`retrieved_docs[0]`)의 일부 내용을 출력합니다.\n"
      ],
      "id": "3a3cb5b8"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "531585d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522bce8f-3531-45f6-8180-75a7222823c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서의 길이: 5733\n",
            "\n",
            "=====================\n",
            "\n",
            " 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
            "연관키워드: 혁신, 기술, 비즈니스 모델\n",
            "\n",
            "Crawling\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "\n",
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n",
            "\n",
            "정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을\n"
          ]
        }
      ],
      "source": [
        "# 검색된 문서의 문서의 페이지 내용의 길이를 출력합니다.\n",
        "print(\n",
        "    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n",
        "    end=\"\\n\\n=====================\\n\\n\",\n",
        ")\n",
        "\n",
        "# 문서의 일부를 출력합니다.\n",
        "print(retrieved_docs[0].page_content[2000:2500])"
      ],
      "id": "531585d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "829387f8"
      },
      "source": [
        "## 더 큰 Chunk 의 크기를 조절\n",
        "\n",
        "이전의 결과처럼 **전체 문서가 너무 커서 있는 그대로 검색하기에는 부적합** 할 수 있습니다.\n",
        "\n",
        "이런 경우, 실제로 우리가 하고 싶은 것은 먼저 원시 문서를 더 큰 청크로 분할한 다음, 더 작은 청크로 분할하는 것입니다.\n",
        "\n",
        "그런 다음 작은 청크들을 인덱싱하지만, 검색 시에는 더 큰 청크를 검색합니다 (그러나 여전히 전체 문서는 아닙니다).\n"
      ],
      "id": "829387f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51dcc001"
      },
      "source": [
        "- `RecursiveCharacterTextSplitter`를 사용하여 부모 문서와 자식 문서를 생성합니다.\n",
        "  - 부모 문서는 `chunk_size`가 1000으로 설정되어 있습니다.\n",
        "  - 자식 문서는 `chunk_size`가 200으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성됩니다.\n"
      ],
      "id": "51dcc001"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "29662c13"
      },
      "outputs": [],
      "source": [
        "# 부모 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
        "# 자식 문서를 생성하는 데 사용되는 텍스트 분할기입니다.\n",
        "# 부모보다 작은 문서를 생성해야 합니다.\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소입니다.\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# 부모 문서의 저장 계층입니다.\n",
        "store = InMemoryStore()"
      ],
      "id": "29662c13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db4a3592"
      },
      "source": [
        "`ParentDocumentRetriever`를 초기화하는 코드입니다.\n",
        "\n",
        "- `vectorstore` 매개변수는 문서 벡터를 저장하는 벡터 저장소를 지정합니다.\n",
        "- `docstore` 매개변수는 문서 데이터를 저장하는 문서 저장소를 지정합니다.\n",
        "- `child_splitter` 매개변수는 하위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n",
        "- `parent_splitter` 매개변수는 상위 문서를 분할하는 데 사용되는 문서 분할기를 지정합니다.\n",
        "\n",
        "`ParentDocumentRetriever`는 계층적 문서 구조를 처리하며, 상위 문서와 하위 문서를 별도로 분할하고 저장합니다. 이를 통해 검색 시 상위 문서와 하위 문서를 효과적으로 활용할 수 있습니다.\n"
      ],
      "id": "db4a3592"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "c3cacb06"
      },
      "outputs": [],
      "source": [
        "retriever = ParentDocumentRetriever(\n",
        "    # 벡터 저장소를 지정합니다.\n",
        "    vectorstore=vectorstore,\n",
        "    # 문서 저장소를 지정합니다.\n",
        "    docstore=store,\n",
        "    # 하위 문서 분할기를 지정합니다.\n",
        "    child_splitter=child_splitter,\n",
        "    # 상위 문서 분할기를 지정합니다.\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ],
      "id": "c3cacb06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc96735d"
      },
      "source": [
        "`retriever` 객체에 `docs`를 추가합니다. `retriever`가 검색할 수 있는 문서 집합에 새로운 문서들을 추가하는 역할을 합니다.\n"
      ],
      "id": "cc96735d"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6f95883b"
      },
      "outputs": [],
      "source": [
        "retriever.add_documents(docs)  # 문서를 retriever에 추가합니다."
      ],
      "id": "6f95883b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0a560f6"
      },
      "source": [
        "이제 문서의 수가 훨씬 더 많아진 것을 볼 수 있습니다. 이는 더 큰 청크(chunk)들입니다.\n"
      ],
      "id": "e0a560f6"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7d0989cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e49ac2-f0d8-4a65-92bb-31535e25de17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# 저장소에서 키를 생성하고 리스트로 변환한 후 길이를 반환합니다.\n",
        "len(list(store.yield_keys()))"
      ],
      "id": "7d0989cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8004388"
      },
      "source": [
        "기본 벡터 저장소가 여전히 작은 청크를 검색하는지 확인해 보겠습니다.\n"
      ],
      "id": "f8004388"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ea50f0"
      },
      "source": [
        "`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n"
      ],
      "id": "b6ea50f0"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ec52ff30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71eade1-641f-4c19-cf16-e2c97a0ff21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n"
          ]
        }
      ],
      "source": [
        "# 유사도 검색을 수행합니다.\n",
        "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
        "# sub_docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\n",
        "print(sub_docs[0].page_content)"
      ],
      "id": "ec52ff30"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "937b2c64"
      },
      "source": [
        "이번에는 `retriever` 객체의 `invoke()` 메서드를 사용하여 문서를 검색합니다.\n"
      ],
      "id": "937b2c64"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9944a690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fa5660-6a58-42c6-8873-6c6bd26412a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n",
            "예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n",
            "연관키워드: 딥러닝, 자연어 처리, Attention\n",
            "\n",
            "HuggingFace\n",
            "\n",
            "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n",
            "예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n",
            "연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
            "\n",
            "Digital Transformation\n",
            "\n",
            "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n",
            "예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
            "연관키워드: 혁신, 기술, 비즈니스 모델\n",
            "\n",
            "Crawling\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "\n",
            "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
            "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
            "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
            "LLM (Large Language Model)\n"
          ]
        }
      ],
      "source": [
        "# 문서를 검색하여 가져옵니다.\n",
        "retrieved_docs = retriever.invoke(\"Word2Vec\")\n",
        "\n",
        "# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\n",
        "print(retrieved_docs[0].page_content)"
      ],
      "id": "9944a690"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCm_Aw1Vwj4I"
      },
      "id": "MCm_Aw1Vwj4I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5bh44Fvwj1c"
      },
      "id": "V5bh44Fvwj1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46198a85"
      },
      "source": [
        "# MultiQueryRetriever\n",
        "\n",
        "거리 기반 벡터 데이터베이스 검색은 고차원 공간에서의 쿼리 임베딩(표현)과 '거리'를 기준으로 유사한 임베딩을 가진 문서를 찾는 방식입니다. 하지만 쿼리의 **세부적인 차이나 임베딩이 데이터의 의미를 제대로 포착하지 못할 경우, 검색 결과가 달라질 수** 있습니다. 또한, 이를 수동으로 조정하는 프롬프트 엔지니어링이나 튜닝 작업은 번거로울 수 있습니다.\n",
        "\n",
        "이런 문제를 해결하기 위해, `MultiQueryRetriever` 는 주어진 사용자 입력 쿼리에 대해 다양한 관점에서 여러 쿼리를 자동으로 생성하는 LLM(Language Learning Model)을 활용해 프롬프트 튜닝 과정을 자동화합니다.\n",
        "\n",
        "이 방식은 각각의 쿼리에 대해 관련 문서 집합을 검색하고, 모든 쿼리를 아우르는 고유한 문서들의 합집합을 추출해, 잠재적으로 관련된 더 큰 문서 집합을 얻을 수 있게 해줍니다.\n",
        "\n",
        "여러 관점에서 동일한 질문을 생성함으로써, `MultiQueryRetriever` 는 거리 기반 검색의 제한을 일정 부분 극복하고, 더욱 풍부한 검색 결과를 제공할 수 있습니다."
      ],
      "id": "46198a85"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "dae75cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b197d2-b57e-4fb6-b794-e060b63cbdf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# 샘플 벡터DB 구축\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 블로그 포스트 로드\n",
        "loader = WebBaseLoader(\n",
        "    \"https://teddylee777.github.io/openai/openai-assistant-tutorial/\", encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# 문서 분할\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "docs = loader.load_and_split(text_splitter)\n",
        "\n",
        "# 임베딩 정의\n",
        "openai_embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 벡터DB 생성\n",
        "db = FAISS.from_documents(docs, openai_embedding)\n",
        "\n",
        "# retriever 생성\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "# 문서 검색\n",
        "query = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# 검색된 문서의 개수 출력\n",
        "len(relevant_docs)"
      ],
      "id": "dae75cb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43d88fd7"
      },
      "source": [
        "검색된 결과 중 1개 문서의 내용을 출력합니다.\n"
      ],
      "id": "43d88fd7"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "d8ea9548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e5d2ac-b309-463a-abba-dbf655a48377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 강력한 도구로서, Assistant에게 사용자 정의 함수를 지정할 수 있습니다. 이는 Chat Completions API에서의 함수 호출과 매우 유사합니다.\n",
            "\n",
            "\n",
            "Function calling(함수 호출) 도구를 사용하면 Assistant 에게 사용자 정의 함수 를 설명하여 호출해야 하는 함수를 인자와 함께 지능적으로 반환하도록 할 수 있습니다.\n",
            "\n",
            "\n",
            "Assistant API는 실행 중에 함수를 호출할 때 실행을 일시 중지하며, 함수 호출 결과를 다시 제공하여 Run 실행을 계속할 수 있습니다. (이는 사용자 피드백을 받아 재게할 수 있는 의미이기도 합니다. 아래 튜토리얼에서 상세히 다룹니다).\n"
          ]
        }
      ],
      "source": [
        "# 1번 문서를 출력합니다.\n",
        "print(relevant_docs[1].page_content)"
      ],
      "id": "d8ea9548"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f3224e"
      },
      "source": [
        "## 사용방법\n",
        "\n",
        "`MultiQueryRetriever` 에 사용할 LLM을 지정하고 질의 생성에 사용하면, retriever가 나머지 작업을 처리합니다.\n"
      ],
      "id": "f8f3224e"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "1637815b"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# ChatOpenAI 언어 모델을 초기화합니다. temperature는 0으로 설정합니다.\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "\n",
        "multiquery_retriever = MultiQueryRetriever.from_llm(  # MultiQueryRetriever를 언어 모델을 사용하여 초기화합니다.\n",
        "    # 벡터 데이터베이스의 retriever와 언어 모델을 전달합니다.\n",
        "    retriever=db.as_retriever(),\n",
        "    llm=llm,\n",
        ")"
      ],
      "id": "1637815b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd0b647"
      },
      "source": [
        "아래는 다중 쿼리를 생성하는 중간 과정을 디버깅하기 위하여 실행하는 코드입니다.\n",
        "\n",
        "먼저 `\"langchain.retrievers.multi_query\"` 로거를 가져옵니다.\n",
        "\n",
        "이는 `logging.getLogger()` 함수를 사용하여 수행됩니다. 그 다음, 이 로거의 로그 레벨을 `INFO`로 설정하여, `INFO` 레벨 이상의 로그 메시지만 출력되도록 할 수 있습니다.\n"
      ],
      "id": "bbd0b647"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "901d1749"
      },
      "outputs": [],
      "source": [
        "# 쿼리에 대한 로깅 설정\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "id": "901d1749"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfda8ebb"
      },
      "source": [
        "이 코드는 `retriever_from_llm` 객체의 `invoke` 메서드를 사용하여 주어진 `question`과 관련된 문서를 검색합니다.\n",
        "\n",
        "검색된 문서들은 `unique_docs`라는 변수에 저장되며, 이 변수의 길이를 확인함으로써 검색된 관련 문서의 총 개수를 알 수 있습니다. 이 과정을 통해 사용자의 질문에 대한 관련 정보를 효과적으로 찾아내고 그 양을 파악할 수 있습니다.\n"
      ],
      "id": "dfda8ebb"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "e2e305f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff1b52a-4db6-40a6-c138-4751acdf6b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['OpenAI Assistant API에서 Functions 기능을 사용하는 방법에 대해 설명해 주세요.', 'OpenAI Assistant API의 Functions를 활용하는 방법에 대한 정보를 제공해 주실 수 있나요?', 'OpenAI Assistant API의 Functions 사용법과 관련된 자료를 찾고 있습니다. 어떤 내용을 참고하면 좋을까요?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============\n",
            "검색된 문서 개수: 5\n",
            "===============\n",
            "OpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.\n",
            "\n",
            "\n",
            "\n",
            "주요내용\n"
          ]
        }
      ],
      "source": [
        "# 질문을 정의합니다.\n",
        "question = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n",
        "# 문서 검색\n",
        "relevant_docs = multiquery_retriever.invoke(question)\n",
        "\n",
        "# 검색된 고유한 문서의 개수를 반환합니다.\n",
        "print(\n",
        "    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n",
        "    end=\"\\n===============\\n\",\n",
        ")\n",
        "\n",
        "# 검색된 문서의 내용을 출력합니다.\n",
        "print(relevant_docs[0].page_content)"
      ],
      "id": "e2e305f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81695892"
      },
      "source": [
        "## LCEL Chain 활용하는 방법\n",
        "\n",
        "- 사용자 정의 프롬프트 정의하고, 정의한 프롬프트와 함께 Chain 을 생성합니다.\n",
        "- Chain 은 사용자의 질문을 입력 받으면 (아래의 예제에서는) 5개의 질문을 생성한 뒤 `\"\\n\"` 구분자로 구분하여 생성된 5개 질문을 반환합니다.\n"
      ],
      "id": "81695892"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "0ab98687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f461e2df-6d7b-4242-b435-b5a23efe4583"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OpenAI Assistant API의 Functions 활용 방법에 대해 설명해 주세요.  \\nOpenAI Assistant API에서 Functions를 사용하는 절차를 알려주세요.  \\nOpenAI Assistant API의 Functions 기능에 대한 안내를 부탁드립니다.  \\nOpenAI Assistant API의 Functions 사용에 대한 정보를 제공해 주세요.  \\nOpenAI Assistant API에서 Functions를 어떻게 활용할 수 있는지 궁금합니다.  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 프롬프트 템플릿을 정의합니다.(5개의 질문을 생성하도록 프롬프트를 작성하였습니다)\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are an AI language model assistant.\n",
        "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.\n",
        "By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
        "Your response should be a list of values separated by new lines, eg: `foo\\nbar\\nbaz\\n`\n",
        "\n",
        "#ORIGINAL QUESTION:\n",
        "{question}\n",
        "\n",
        "#Answer in Korean:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 언어 모델 인스턴스를 생성합니다.\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "\n",
        "# LLMChain을 생성합니다.\n",
        "custom_multiquery_chain = (\n",
        "    {\"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 질문을 정의합니다.\n",
        "question = \"OpenAI Assistant API의 Functions 사용법에 대해 알려주세요.\"\n",
        "\n",
        "# 체인을 실행하여 생성된 다중 쿼리를 확인합니다.\n",
        "multi_queries = custom_multiquery_chain.invoke(question)\n",
        "# 결과를 확인합니다.(5개 질문 생성)\n",
        "multi_queries"
      ],
      "id": "0ab98687"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c6403eb"
      },
      "source": [
        "이전에 생성한 Chain을 `MultiQueryRetriever` 에 전달하여 retrieve 할 수 있습니다.\n"
      ],
      "id": "0c6403eb"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5f3cac81"
      },
      "outputs": [],
      "source": [
        "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
        "    llm=custom_multiquery_chain, retriever=db.as_retriever()\n",
        ")"
      ],
      "id": "5f3cac81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086076bb"
      },
      "source": [
        "`MultiQueryRetriever`를 사용하여 문서를 검색하고 결과를 확인합니다.\n"
      ],
      "id": "086076bb"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "6eaffe30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6bed01-fb76-40f4-c824-39735d3c996e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['OpenAI Assistant API의 Functions 사용법에 대해 설명해 주세요.  ', 'OpenAI Assistant API에서 Functions를 사용하는 방법이 궁금합니다.  ', 'OpenAI Assistant API의 Functions 기능을 어떻게 활용할 수 있나요?  ', 'OpenAI Assistant API의 Functions에 대한 사용 지침을 제공해 주세요.  ', 'OpenAI Assistant API에서 Functions를 사용하는 절차를 알려주세요.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============\n",
            "검색된 문서 개수: 4\n",
            "===============\n",
            "OpenAI의 새로운 Assistants API는 대화와 더불어 강력한 도구 접근성을 제공합니다. 본 튜토리얼은 OpenAI Assistants API를 활용하는 내용을 다룹니다. 특히, Assistant API 가 제공하는 도구인 Code Interpreter, Retrieval, Functions 를 활용하는 방법에 대해 다룹니다. 이와 더불어 파일을 업로드 하는 내용과 사용자의 피드백을 제출하는 내용도 튜토리얼 말미에 포함하고 있습니다.\n",
            "\n",
            "\n",
            "\n",
            "주요내용\n"
          ]
        }
      ],
      "source": [
        "# 결과\n",
        "relevant_docs = multiquery_retriever.invoke(question)\n",
        "\n",
        "# 검색된 고유한 문서의 개수를 반환합니다.\n",
        "print(\n",
        "    f\"===============\\n검색된 문서 개수: {len(relevant_docs)}\",\n",
        "    end=\"\\n===============\\n\",\n",
        ")\n",
        "\n",
        "# 검색된 문서의 내용을 출력합니다.\n",
        "print(relevant_docs[0].page_content)"
      ],
      "id": "6eaffe30"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wt_nCvmKxI3D"
      },
      "id": "wt_nCvmKxI3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9l6RVr20xI0d"
      },
      "id": "9l6RVr20xI0d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}